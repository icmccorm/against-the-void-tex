\subsubsection{Invariants}
Participants reasoned about the soundness and correctness of their \unsafe code in terms of invariants.  When developing operating systems and embedded applications, certain invariants were provided by hardware specifications. For example, one participant who developed an embedded application would write a \ilquote{magic value}{8} to RAM to indicate which operation had triggered a reboot. The microcontroller that they used did not erase its memory on reboot, so they could rely on the value being initialized. Some participants perceived hardware-level correctness properties as \ilquote{orthogonal to Rust}{1}, since they are not related to the properties of the language. However, other participants who developed embedded applications relied on Rust libraries that encode hardware requirements into the type system so that if a device is misconfigured, then their program will not compile.

Developers also leveraged Rust's type system to uphold invariants that they perceived as necessary for soundness. In operating systems, these typically began as operation-specific refinement properties on primitive types, but they quickly built up into system-level invariants; \ilquote{it gets to a high-level really quickly}{3}. Rust's aliasing restrictions were a natural fit:
\begin{pquote}{12}
We only allow... someone who has access to one of these mapped memory regions, to borrow it... That's a great example of taking something that's unsafe inherently and then kind of wrapping it up in a safe abstraction, using the power of Rust to do the borrow checking for us.
\end{pquote}
Another participant experimented with using ghost permission to reason about pointer arithmetic; this method was popularized for Rust by Yanovski et al.'s \code{GhostCell} type~\cite{ghostcell21}.

Other participants relied on ad-hoc reasoning to ensure that their programs were correct and free of undefined behavior. Some appealed to their prior experience using \CC{}; \ilquote{{\normalfont [}in{\normalfont ]} \CC{}, you do this all the time}{15}. One participant was accustomed to reasoning about aliasing rules in compiler development, so they felt confident that they were adhering to Rust's restrictions in \unsafe contexts. Other participants validated their decisions by auditing their code, but most had some level of implicit trust; \ilquote{that's sort of another... I got to trust it type thing}{5}. This aligns with {Höltervennhoff et al.~\cite{holtervennhoff23}\textemdash a majority of their participants reasoned about \unsafe code in terms of contracts or invariants, but some reported using it carelessly, and most felt that it was difficult to write correctly due to its context-sensitivity.

\subsubsection{Isolation}
The majority of interview participants made a conscious effort to minimize and isolate \unsafe code, which made it easier to document and reason about.
\begin{pquote}{10}
That's generally the most important thing, just being self-contained, being well-isolated, being well-encapsulated...
\end{pquote}


\FPeval{\understoodeasily}{round(\ArrayItem{rq3.understand.somewhat.easy} + \ArrayItem{rq3.understand.extremely.easy}, 0)}
\FPeval{\rarelyrefactored}{round(\ArrayItem{rq3.refactored.sometimes}, 0)}
\FPeval{\neverrefactored}{round(\ArrayItem{rq3.refactored.never}, 0)}
Most survey respondents (\understoodeasily\%) predicted that it would be at least somewhat easy for another developer at their skill level to understand a random \unsafe block or function from their code. Interview participants who isolated their \unsafe code had more confidence that its requirements were met, especially if they went beyond what Rust's type system could verify. However, participants who contributed JIT compilers and operating systems reported that \unsafe code was prevalent. Domain-specific, non-local reasoning was necessary to understand any arbitrary \unsafe code snippet: \ilquote{you have to know the innards of the JIT-compiled function...so it's just not encapsulated}{11}. 
Though \unsafe code may have been isolated, it is unclear if participants used it minimally. Among survey respondents, \rarelyrefactored\% only sometimes refactored their code to remove \unsafe, while \neverrefactored\% never did at all.

\subsubsection{Encapsulation}
Most interview participants and \ArrayItemRounded{rq3.api.safe.yes}\%
of survey respondents attempted to encapsulate \unsafe code beneath safe APIs. This was also common for Höltervennhoff et al.~\cite{holtervennhoff23}, who found that 22 of their 26 participants mentioned safe interfacing. Our interview participants would create encapsulations where they perceived that the requirements for their \unsafe code were satisfied by either Rust's type system or what they reasoned to be local invariants. However \ArrayItemRounded{rq3.api.unsafe.yes}\% of survey respondents reported exposing \unsafe APIs, suggesting that it may be difficult to avoid. In contrast, only 6 of Holtervenhoff et al.~\cite{holtervennhoff23}'s participants indicated exposing an \unsafe interface.

Interview participants cited the same motivations for exposing \unsafe APIs as they did for using \unsafe code in any capacity. One interview participant exposed unsafe versions of safe API endpoints, which behaved the same as their safe counterparts, but they did not include runtime checks. Another participant exposed \unsafe APIs to allow users to circumvent their safe encapsulations in case they became difficult to use. One participant indicated that it was a cultural value among Rust developers to make API surfaces as detailed as possible: \ilquote{Rust has a tendency to... make the API as complicated as it needs to be to fully represent what is actually happening behind the scenes}{1}. Other participants indicated that they would only expose an \unsafe API when it would be impossible to encapsulate without placing the burden of correctness on the user. Höltervennhoff et al.~\cite{holtervennhoff23} also found that participants would expose \unsafe APIs by necessity or to improve performance, but it was unclear if ergonomics was also a motivation.

Our survey respondents also identified with these motivations for exposing \unsafe APIs. The majority (\ArrayItemRounded{rq3.unsafe.api.motivation.impossible.to.encapsulate.without.imposing.safety.requirements.on.the.user}\%) exposed \unsafe APIs because it would be impossible to encapsulate them without preconditions, while \ArrayItemRounded{rq3.unsafe.api.motivation.to.provide.a.more.performant.equivalent.of.a.safe.api}\% exposed \unsafe APIs for performance and \ArrayItemRounded{rq3.unsafe.api.motivation.to.provide.a.more.ergonomic.equivalent.of.a.safe.api}\% did so for ergonomics. Respondents typically documented the safety requirements of their \unsafe APIs\textemdash \ArrayItemRounded{rq3.unsafe.preconditions.always}\% of respondents only exposed \unsafe APIs when they had requirements beyond Rust's type system, and \ArrayItemRounded{rq3.documented.unsafe.always}\% would always document these requirements.

\subsubsection{Uncertainty}
Developers who created safe APIs for \unsafe code were uncertain if their encapsulations were sound in all possible situations.

\begin{pquote}{9}
No library author knows when they're going to trigger [undefined behavior]... we do our best to give you a sound interface, but god knows if there's a hole in it...
\end{pquote}

Only \ArrayItemRounded{rq3.safe.preconditions.always}\% of survey respondents were always certain that their safe encapsulations met the requirements of the \unsafe code within. Höltervennhoff et al.~\cite{holtervennhoff23} had similar findings; 16 of their 26 participants felt some degree of uncertainty about whether their \unsafe code was correct. Our interview participants were most often uncertain when \unsafe was pervasive or when using Rust's Foreign Function Interface (FFI). This is a significant contrast from Holtervennhoff et al.~\cite{holtervennhoff23}, who found that it was more common for participants perceive foreign functions as easy to encapsulate.

When describing their uncertainty about encapsulation, our participants compared the ``theory'' of safety to the ``practice'' of how an API would be used. When these concepts didn't align, errors occurred. One participant observed this mismatch with a Rust crate that encapsulated a foreign library. The library used a memory-mapped file, and the API could unmap the file while the user retained a reference to it, leading to a use-after-free error. Another participant struggled with discrepancies between the interface and the implementation of Rust's \code{Layout} API, which describes the size and alignment of an allocation. The design of  \code{Layout} was changed to \ilquote{tweak the safety requirements}{14}, which regretfully caused previously correct code to exhibit undefined behavior. 
\label{response:rq2}

\rsqthree Participants derived requirements for \unsafe code from external specifications and the inherent guarantees of the borrow checker. However, they predominantly relied on ad-hoc reasoning and tacit knowledge and were uncertain if their safe encapsulations were sound in all situations. Participants attempted to minimize and isolate \unsafe code, but this was not always possible in certain domain-specific contexts, such as JIT compilers and operating systems. Additionally, when participants chose to expose \unsafe APIs, we found that they were motivated by performance, ergonomics, and necessity to the same extent as for any arbitrary use of \unsafe code.

