We adapted a sequential exploratory mixed-methods design~\cite{mixed_methods_designs}. This consisted of semi-structured interviews with 19 developers. We used our findings to create a survey, which received \ArrayItem{responses.survey.valid} valid responses. We chose this method to obtain a large sample size, which allowed us to assess how our qualitative results generalized to a broader set of developers who use \unsafe code.

\subsection{Interviews} 
On May 3rd, 2023, we posted a call to participate in semi-structured interviews on the Rust subreddit (\href{https://www.reddit.com/r/rust/}{/r/rust}), the Rust Programming Language forums, and the \verb|#dark-arts| channel of the Rust Community Discord server. We also contacted acquaintances with relevant experience. To be eligible to participate, candidates needed to have at least one year of experience using Rust and had to have ``regularly written or edited Rust code within an \unsafe block or function.'' Eligible candidates completed a series of multiple choice and short answer questions about their use of \unsafe features and development tools, and we used their responses to guide each interview. We also collected participants' years of experience with Rust and asked them to describe their affiliation. The first and third authors collaboratively coded each affiliation as ``Industry,'' ``Rust Team,'' ``Academia,'' and ``Open Source.''

The first author conducted all 19 interviews remotely over Zoom between May and July 2023. We used snowball sampling~\cite{patton1990qualitative} to expand the pool of participants beyond the group that responded to our initial calls to participate. If a participant shared new information that we had not yet learned from prior interviews, we prompted them to invite any acquaintances with relevant experience to complete our screening survey. We stopped recruitment once we had reached saturation. Audio recordings were transcribed using Whisper~\cite{whisper}, and the first author reviewed each transcript to correct errors and label whether responses came from the interviewer or the interviewee.

\subsection{Qualitative Analysis}
We conducted an inductive, thematic analysis of our interview transcripts following best practices by Miles, Huberman, and Salda√±a~\cite{miles20}. This consisted of an open coding phase followed by closed coding and auditing. 

\paragraph{Open Coding} In the open coding phase, the first, second, and third authors independently coded random samples of quotes. Each sample contained 19 quotes, with one chosen from each interview. Throughout this process, each coder also reviewed the full text of each interview transcript. We stopped when we had generated 202 unique codes from three random sets of quotes. Then, we collaboratively conducted second-cycle coding, grouping subsets of codes under themes. To refine these themes, the first and second authors conducted closed coding of 7 additional random samples of quotes, meeting after coding each sample as before to resolve disagreements.

\paragraph{Closed Coding \& Auditing} Then, the first and second authors divided the set of transcripts between each other to code in full, independently, using ATLAS.ti Web~\cite{atlasti}. The first author coded 11 transcripts, while the second author coded the remaining 8 transcripts. Throughout this process, the first and second authors updated a shared codebook with additional codes and themes to describe new observations. After all transcripts had been coded once, the first author conducted an additional coding pass for every transcript and resolved all of their disagreements with the second author. The third author reviewed all coding decisions relevant to RQ1 and resolved additional disagreements with the first author. The final codebook includes 191 codes grouped under 21 themes. 

\subsection{Survey}
We used the themes from our qualitative analysis to create a survey in Qualtrics~\cite{qualtrics}. The first, second, and third authors each created sets of potential questions, and we assembled these into a survey organized by sections corresponding to themes and research questions. Two of our interview participants piloted an initial draft, and we incorporated their feedback into the final version.

We relaxed the eligibility criteria for the survey to include the population of Rust developers studied by Holtervennhoff et al.~\cite{holtervennhoff23}. We felt that this was reasonable, since our interview population was a strict subset of theirs; they interviewed developers who had committed \unsafe code to a public GitHub repository, but there was no indication if they had done so regularly. Candidates were eligible to complete our survey if they had ``written, edited, read, audited, or engaged with unsafe Rust code in any way.'' 

We distributed our community survey on September 22nd, 2023 to the same communities and acquaintances as we did for the screening survey. We also published a link in the September 28th edition of the ``This Week in Rust'' newsletter, and we advertised the survey during a presentation and poster sessions at an industry partners conference hosted by our institution. At the end of the survey, participants could provide a link to their profile on either GitHub or the Rust Programming Language Forums to have a chance to receive one of two \$250 gift cards. Only participants with public account activity prior to the start date of the survey were eligible to receive compensation. We closed the survey on October 9th, 2023.

\subsection{Ethics} 
All procedures in this study were reviewed and approved by our institution's IRB. Prior to each intervention, participants were read or presented a script outlining the procedures of the study and they were asked to affirm consent. Participants were reminded to avoid sharing personally identifying information in their responses, and interview participants were asked to remain in a private space. The first author reviewed the contents of the replication package and redacted all direct (e.g. name, affiliation) and indirect (e.g. position, project) personal identifiers.

\subsection{Threats to Validity} 
We used a mixed methods design to improve \textit{construct validity} and audited our final coding decisions to ensure consistency. The \textit{internal validity} of our results depends on participants' ability to accurately self-report their behavior, and what developers perceive about their use of \unsafe code may differ from how \unsafe is used in practice. \textit{External validity} and generalizability also remain threats to validity, which is common for research studies that use surveys and interviews~\cite{Yin09}. We configured Qualtrics to reject multiple responses from the same individual and to detect fraudulent responses using both RelevantID~\cite{relevantid} and reCaptcha~\cite{liu18}. Recruitment via snowball sampling and through community forums may bias our results toward certain areas of the Rust community. Additionally, the gift cards that we offered for compensation are most easily redeemable in North America and Europe, which may have dissuaded certain individuals from participating. We did not collect location data from participants, so we cannot determine if our results are biased toward individuals from any particular area.

\subsection{Replication}
Our replication package is publicly available on GitHub \footnote {github.com/icmccorm/against-the-void/}. It contains all interview transcripts, survey responses, codebooks, and coding decisions.